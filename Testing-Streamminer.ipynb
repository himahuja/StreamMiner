{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6bf67a59-4c96-470e-bdb1-0b32bae5848f"
    }
   },
   "source": [
    "# Import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "4d73c679-2f35-4898-8979-4dbdb11dc2a0"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory not found: /home/alex/Documents/streamminer/data\n",
      "Download data per instructions on:\n",
      "\thttps://github.com/shiralkarprashant/knowledgestream#data\n",
      "and enter the directory path below.\n",
      "\n",
      "Please enter data directory path: /home/alex/Documents/Research/StreamMiner/StreamMiner\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-75b388d4fd08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# raise Exception('Please set HOME to data directory in algorithms/__main__.py')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHOME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kg/_undir/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mSHAPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m6060993\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6060993\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m663\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mWTFN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'logdegree'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import ujson as json\n",
    "import logging as log\n",
    "\n",
    "from pandas import DataFrame, Series\n",
    "from os.path import expanduser, abspath, isfile, isdir, basename, splitext, \\\n",
    "\tdirname, join, exists\n",
    "from time import time\n",
    "from datetime import date\n",
    "import cPickle as pkl\n",
    "\n",
    "#####################################\n",
    "from datastructures.rgraph import Graph, weighted_degree\n",
    "#####################################\n",
    "\n",
    "\n",
    "from time import time\n",
    "from os.path import exists, join, abspath, expanduser, basename, dirname, \\\n",
    "\tisdir, splitext\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from datastructures.rgraph import make_graph, Graph\n",
    "from datastructures.relationalpath import RelationalPath\n",
    "from datastructures.relationalpath_sm import RelationalPathSM\n",
    "from pathenum import get_paths as c_get_paths\n",
    "## for streamminer,\n",
    "from pathenum import get_paths_sm as c_get_paths_sm\n",
    "\n",
    "##############################################\n",
    "from algorithms.mincostflow.ssp import succ_shortest_path, disable_logging\n",
    "from algorithms.relklinker.rel_closure import relational_closure as relclosure\n",
    "from algorithms.klinker.closure import closure\n",
    "##############################################\n",
    "\n",
    "###################################################################\n",
    "################# DATABASE and RELSIM SETUP #######################\n",
    "###################################################################\n",
    "# KG - DBpedia\n",
    "HOME = abspath(expanduser('~/Documents/streamminer/data/'))\n",
    "if not exists(HOME):\n",
    "\tprint 'Data directory not found: %s' % HOME\n",
    "\tprint 'Download data per instructions on:'\n",
    "\tprint '\\thttps://github.com/shiralkarprashant/knowledgestream#data'\n",
    "\tprint 'and enter the directory path below.'\n",
    "\tdata_dir = raw_input('\\nPlease enter data directory path: ')\n",
    "\tif data_dir != '':\n",
    "\t\tdata_dir = abspath(expanduser(data_dir))\n",
    "\tif not os.path.isdir(data_dir):\n",
    "\t\traise Exception('Entered path \"%s\" not a directory.' % data_dir)\n",
    "\tif not exists(data_dir):\n",
    "\t\traise Exception('Directory does not exist: %s' % data_dir)\n",
    "\tHOME = data_dir\n",
    "\t# raise Exception('Please set HOME to data directory in algorithms/__main__.py')\n",
    "PATH = join(HOME, 'kg/_undir/')\n",
    "assert exists(PATH)\n",
    "SHAPE = (6060993, 6060993, 663)\n",
    "WTFN = 'logdegree'\n",
    "\n",
    "# # relational similarity using TF-IDF representation and cosine similarity\n",
    "# RELSIMPATH = join(HOME, 'relsim/coo_mat_sym_2016-10-24_log-tf_tfidf.npy')\n",
    "# assert exists(RELSIMPATH)\n",
    "##############################################################\n",
    "RELSIMPATH = join(HOME, 'relsim/coo_mat_sym_2016-10-24_log-tf_tfidf.npy')\n",
    "assert exists(RELSIMPATH)\n",
    "##############################################################\n",
    "# relational similarity using TF-IDF representation and cosine similarity\n",
    "\n",
    "# relsim = np.load(RELSIMPATH)\n",
    "\n",
    "# Date\n",
    "DATE = '{}'.format(date.today())\n",
    "\n",
    "# data types for int and float\n",
    "_short = np.int16\n",
    "_int = np.int32\n",
    "_int64 = np.int64\n",
    "_float = np.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d77ac976-3df5-4f75-a8ee-e7e6d343fc9a"
    }
   },
   "outputs": [],
   "source": [
    "\trelsim = np.load(RELSIMPATH)\n",
    "\n",
    "\toutdir = abspath(expanduser('output/'))\n",
    "\tassert exists(outdir)\n",
    "\tdatafile = abspath(expanduser('datasets/sample.csv'))\n",
    "\tassert exists(datafile)\n",
    "\tlog.info('Launching {}..'.format('SM'))\n",
    "\tlog.info('Dataset: {}'.format(basename(datafile)))\n",
    "\tlog.info('Output dir: {}'.format(outdir))\n",
    "\n",
    "\t# read data\n",
    "\tdf = pd.read_table(datafile, sep=',', header=0)\n",
    "\tlog.info('Read data: {} {}'.format(df.shape, basename(datafile)))\n",
    "\tspo_df = df.dropna(axis=0, subset=['sid', 'pid', 'oid'])\n",
    "\tlog.info('Note: Found non-NA records: {}'.format(spo_df.shape))\n",
    "\tdf = spo_df[['sid', 'pid', 'oid']].values\n",
    "\tsubs, preds, objs  = df[:,0].astype(_int), df[:,1].astype(_int), df[:,2].astype(_int)\n",
    "\n",
    "\t# load knowledge graph\n",
    "\tG = Graph.reconstruct(PATH, SHAPE, sym=True) # undirected\n",
    "\tassert np.all(G.csr.indices >= 0)\n",
    "    \n",
    "\tbase = splitext(basename(datafile))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some external test settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Experiments on the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Shape of the data array: {}\".format(G.csr.data.shape) #Undirected graph\n",
    "print \"Shape of the indices array: {}\".format(G.csr.indices.shape) # for all the data points, there is one\n",
    "print \"Shape of the indptr array: {}\".format(G.csr.indptr.shape) # for all the 6M entities\n",
    "# To get an item on a particular position, just perform indexing on the self.csr matrix [node_row, rel_num * G.N + col_num]\n",
    "plt.hist(G.csr.data, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some settings for SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = spo_df\n",
    "y = triples['class'] # ground truth\n",
    "triples = triples[['sid', 'pid', 'oid']].to_dict(orient='records')\n",
    "\n",
    "pid_removal = triples[0]['pid']\n",
    "print 'PID is: {}, with type: {}'.format(pid_removal, pid_removal.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_degree(arr, weight='logdegree'):\n",
    "\t\"\"\"Returns a weighted version of the array.\"\"\"\n",
    "\tif weight == 'degree':\n",
    "\t\tarr = 1./(1 + arr)\n",
    "\telif weight == 'logdegree':\n",
    "\t\tarr = 1./(1 + np.log(arr))\n",
    "\telse:\n",
    "\t\traise ValueError('Unknown weight function.')\n",
    "\treturn arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indegsim = weighted_degree(G.indeg_vec, weight=WTFN).reshape((1, G.N))\n",
    "indegsim = indegsim.ravel()\n",
    "targets = G.csr.indices % G.N #target nodes are the node values that would occur in the first E x E matrix\n",
    "specificity_wt = indegsim[targets]\n",
    "print \"Specificity_wt has the shape: {}\".format(specificity_wt.shape)\n",
    "plt.hist(specificity_wt, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the relsim vector corresponding to predicate, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = (G.csr.indices - targets) / G.N #gives the respective relation values of those indices\n",
    "relsimvec = np.array(relsim[int(pid_removal), :]) # specific to predicate p\n",
    "print \"Expected shape of the relsimvec is: {}\".format(relsimvec.shape)\n",
    "relsim_wt = relsimvec[relations] # with the size of relations as the number of relations\n",
    "print \"Shape of relsim_wt is: {}\".format(relsim_wt.shape)\n",
    "# Binning the cosine similarity plot\n",
    "plt.hist(relsimvec,density=1, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the edge corresponding to p in the specificity matrix (soon to be data matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print '=> Removing predicate {} from KG.'.format(pid_removal)\n",
    "eraseedges_mask = ((G.csr.indices - (G.csr.indices % G.N)) / G.N) == pid_removal\n",
    "specificity_wt[eraseedges_mask] = 0\n",
    "relsim_wt[eraseedges_mask] = 0\n",
    "plt.hist(specificity_wt, density = 1, bins=100, label='specificity')\n",
    "plt.hist(relsim_wt, density = 1, bins=100, label='relsim wrt to, {}'.format(pid_removal))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting values changed in graph, setting backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.csr.data = specificity_wt.copy()\n",
    "plt.hist(G.csr.data, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performing operation for a single (s, p, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_sm_limited(G, s, p, o, relsim_wt, weight = 10.0, maxpaths=-1, top_n_neighbors=5):\n",
    "\t# \"Returns all paths of length `length` starting at s and ending in o.\"\n",
    "\tpath_stack = [[s]]\n",
    "\tweight_path_stack = [[0.0]]\n",
    "\trelpath_stack = [[-1]]\n",
    "\tdiscoverd_paths = []\n",
    "\tprint 'We\\'re checking the path: ({}, {}, {})'.format(s, p, o)\n",
    "\twhile len(path_stack) > 0:\n",
    "\t\t# print 'Stack: {} {}'.format(path_stack, relpath_stack)\n",
    "\t\tcurr_path = path_stack.pop()\n",
    "\t\tcurr_relpath = relpath_stack.pop()\n",
    "\t\tnode = curr_path[-1]\n",
    "\t\tcurr_path_weight = weight_path_stack.pop()\n",
    "\t\t# print 'Node: {}'.format(node)\n",
    "\t\ttotal_path_weight = np.sum(curr_path_weight)\n",
    "\t\tif total_path_weight <= weight:\n",
    "\t\t\tif int(node) == int(o):\n",
    "\t\t\t\tprint 'The total weight of the path is: {}'.format(np.sum(curr_path_weight))\n",
    "\t\t\t\tpath = RelationalPathSM(\n",
    "\t\t\t\t\ts, p, o, 0., len(curr_path)-1, curr_path, curr_relpath, curr_path_weight\n",
    "\t\t\t\t)\n",
    "\t\t\t\tdiscoverd_paths.append(path)\n",
    "\t\t\t\tif maxpaths != -1 and len(discoverd_paths) >= maxpaths:\n",
    "\t\t\t\t\tprint \"Exceeded number of paths!\"\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tcontinue\n",
    "\t\telif total_path_weight > weight:\n",
    "\t\t\t#print 'Discarded path with weight'\n",
    "\t\t\tcontinue\n",
    "\t\t# print \"Node is: {}, o is: {}, s is: {}, p is: {}\".format(node, o, s, p)\n",
    "\t\trelnbrs, data = G.get_neighbors_sm(int(node))\n",
    "\t\tordering = np.argsort(data)\n",
    "\t\trelnbrs = relnbrs[:, ordering]\n",
    "\t\t# print 'Data vector is: {}'.format(data)\n",
    "\t\t# print(data)\n",
    "\t\trange = relnbrs.shape[1] if relnbrs.shape[1] < top_n_neighbors else top_n_neighbors\n",
    "\t\tfor i in xrange(range):\n",
    "\t\t\trel, nbr = relnbrs[:, i]\n",
    "\t\t\t# print \"rel is: {}, nbr is: {}\".format(rel, nbr)\n",
    "\t\t\tpath_stack.append(curr_path + [nbr])\n",
    "\t\t\tweight_path_stack.append(curr_path_weight + [data[i]])\n",
    "\t\t\trelpath_stack.append(curr_relpath + [rel])\n",
    "\treturn discoverd_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_spo_test(G, idx, triples, y, relsim_wt, features=None):\n",
    "\t# Hyperparameter Setting:\n",
    "\treturn_features = False\n",
    "\tif features is None:\n",
    "\t\treturn_features = True\n",
    "\t\tfeatures, pos_features, neg_features = set(), set(), set()\n",
    "\tmeasurements = []\n",
    "\tG_bak = {\n",
    "\t'data': G.csr.data.copy(),\n",
    "\t'indices': G.csr.indices.copy(),\n",
    "\t'indptr': G.csr.indptr.copy()\n",
    "\t}\n",
    "\ttriple = triples[idx]\n",
    "\tsid, pid, oid = triple['sid'], triple['pid'], triple['oid']\n",
    "\tlabel = y[idx]\n",
    "\ttargets = G.csr.indices % G.N #shift this function to the caller\n",
    "\tG.csr.data[targets == oid] = 1 # no cost for target t => max. specificity.\n",
    "\tG.csr.data = np.multiply(relsim_wt, G.csr.data)\n",
    "\tplt.hist(G.csr.data, density = 1, bins=100)\n",
    "\tplt.show()\n",
    "\t# PERFORM PATH EXTRACTION\n",
    "\t#paths = get_paths_sm_limited(G, sid, pid, oid, relsim_wt, \\\n",
    "\t\t\t#weight = weight, maxpaths=200, top_n_neighbors=1000)\n",
    "# \trp = relclosure(G, int(sid), int(pid), int(oid), kind='metric', linkpred=False)\n",
    "# \tpaths = {}\n",
    "#     paths['path'] = rp.path\n",
    "\tpaths = yenKSP(G, sid, pid, oid)\n",
    "\tprint \"Extraction complete for, s:{}, p:{}, o:{}\".format(sid, pid, oid)\n",
    "# \tprint '{}'.format(rp.score)\n",
    "# \tfor pth in paths:\n",
    "# \t\tff =  tuple(pth.relational_path)\n",
    "# \t\tif ff not in features:\n",
    "# \t\t\tfeatures.add(ff)\n",
    "# \t\t\tif label == 1:\n",
    "# \t\t\t\tpos_features.add(ff)\n",
    "# \t\t\tif label == 0:\n",
    "# \t\t\t\tneg_features.add(ff)\n",
    "# \t\t\telse:\n",
    "# \t\t\t\traise Exception(\"Unknown class label: {}\".format(label))\n",
    "# \t\ttriple_feature[ff] = triple_feature.get(ff, 0) + 1\n",
    "# \tmeasurements.append(triple_feature)\n",
    "# \tsys.stdout.flush()\n",
    "\t#Restoring backup\n",
    "\tnp.copyto(G.csr.data, G_bak['data'])\n",
    "\tnp.copyto(G.csr.indices, G_bak['indices'])\n",
    "\tnp.copyto(G.csr.indptr, G_bak['indptr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relax(weight, u, v, r, Dist, prev):\n",
    "\td = Dist.get(u, inf) + weight\n",
    "\tif d < Dist.get(v, inf):\n",
    "\t\tDist[v] = d\n",
    "\t\tprev[v] = (-weight, u, r)\n",
    "\n",
    "def get_shortest_path(G, sid, pid, oid):\n",
    "\t#making sure that nodes are integers:\n",
    "\t# discovered_path = []\n",
    "\tsid = int(sid)\n",
    "\toid = int(oid)\n",
    "\t#prev is of the type: [weight, node, relation]\n",
    "\tDist, visited, priority_q, prev = {sid:0}, set(), [(0,sid)], {sid:(0, -1, -1)}\n",
    "\tpath_stack, rel_stack, weight_stack = [], [], []\n",
    "\twhile priority_q:\n",
    "\t\t_, u = heapq.heappop(priority_q)\n",
    "\t\tif u == oid:\n",
    "\t\t\tk = u\n",
    "\t\t\tpath_stack = [oid]\n",
    "\t\t\twhile prev[k][1] != -1:\n",
    "\t\t\t\tpath_stack.insert(0, prev[k][1])\n",
    "\t\t\t\tprint\n",
    "\t\t\t\trel_stack.insert(0, prev[k][2])\n",
    "\t\t\t\tweight_stack.insert(0, prev[k][0])\n",
    "\t\t\t\tk = prev[k][1]\n",
    "\t\t\tbreak\n",
    "\t\tif u in visited:\n",
    "\t\t\tcontinue\n",
    "\t\tvisited.add(u)\n",
    "\t\t# get the neighbours and cost of the node u\n",
    "\t\t# returns [relations, neighbors, cost]\n",
    "\t\trels, nbrs, costs = G.get_neighbors_sm_unpacked(int(u))\n",
    "\t\tfor rel, nbr, cost in zip(rels, nbrs, costs): # for the iteration through keys\n",
    "\t\t\tif cost != 0:\n",
    "\t\t\t\trelax(-cost, u, nbr, rel, Dist, prev)\n",
    "\t\t\t\theapq.heappush(priority_q, (-cost, nbr))\n",
    "\t\t\t\t# discovered_path = RelationalPathSM(sid, pid, oid, 0., len(path_stack)-1, ..)  \t\t\t\t\t\t\t\t  path_stack, rel_stack, weight_stack)\n",
    "\treturn path_stack, rel_stack, weight_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_spo_test(G, 0, triples, y, relsim_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "indentWithTabs": 2
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
